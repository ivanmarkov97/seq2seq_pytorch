# -*- coding: utf-8 -*-
"""simple_seq2seq.ipynb

Automatically generated by Colaboratory.
"""

import os
import random
import numpy as np

import functools

import torch
import torch.nn as nn

from torchtext import datasets
from torchtext.data import Field
from torchtext.data import BucketIterator


SEED = 241

def seed_everything(seed):
  random.seed(seed)
  np.random.seed(seed)
  torch.manual_seed(seed)
  os.environ['PYTHONHASHSEED'] = str(seed)

  if torch.cuda.is_available(): 
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

seed_everything(SEED)

import spacy

!python -m spacy download de --quiet
!python -m spacy download en --quiet

spacy_de = spacy.load('de')
spacy_en = spacy.load('en')

def tokenize_de(text):
  return [token.text for token in spacy_de.tokenizer(text)][::-1]

def tokenize_en(text):
  return [token.text for token in spacy_en.tokenizer(text)]

SRC = Field(lower=True,
            use_vocab=True, 
            sequential=True, 
            init_token='<sos>',
            eos_token='<eos>',
            batch_first=True,
            include_lengths=True,
            tokenize=tokenize_de)

TRG = Field(lower=True, 
            use_vocab=True,
            sequential=True,
            init_token='<sos>',
            eos_token='<eos>',
            batch_first=True,
            include_lengths=True,
            tokenize=tokenize_en)

train_data, valid_data, test_data = datasets.Multi30k.splits(exts=('.de', '.en'),
                                                             fields=[('de', SRC), ('en', TRG)])

SRC.build_vocab(train_data)
TRG.build_vocab(train_data)

batch_size = 32
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


train_iterator = BucketIterator.splits((train_data,), batch_size=batch_size, device=device)[0]
valid_iterator = BucketIterator.splits((valid_data,), batch_size=batch_size, device=device)[0]
test_iterator = BucketIterator.splits((test_data,), batch_size=batch_size, device=device)[0]

class Config:
  def __init__(self,
               vocab_size,
               emb_size,
               hidden_size,
               num_layers, 
               dropout, 
               pad_index,
               device):
    self.vocab_size = vocab_size
    self.emb_size = emb_size
    self.hidden_size = hidden_size
    self.num_layers = num_layers
    self.dropout = dropout
    self.pad_index = pad_index
    self.device = device

class Encoder(nn.Module):

  def __init__(self, config):
    super().__init__()
    self.embedding = nn.Embedding(config.vocab_size,
                                  config.emb_size, 
                                  padding_idx=config.pad_index)
    self.dropout = nn.Dropout(config.dropout)
    self.rnn = nn.LSTM(config.emb_size,
                       config.hidden_size, 
                       config.num_layers, 
                       batch_first=True, 
                       dropout=config.dropout)
    self.device = config.device

  def forward(self, text, text_lens):
    text_embedded = self.dropout(self.embedding(text))
    packed_sequence = nn.utils.rnn.pack_padded_sequence(text_embedded, 
                                                        text_lens,
                                                        batch_first=True, 
                                                        enforce_sorted=False)
    
    packed_outputs, (hidden, cell) = self.rnn(packed_sequence)
    outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)
    # hidden = hidden.permute(1, 0, 2)
    # cell = cell.permute(1, 0, 2)
    return hidden, cell

class Decoder(nn.Module):

  def __init__(self, config):
    super().__init__()
    self.embedding = nn.Embedding(config.vocab_size, 
                                  config.emb_size, 
                                  padding_idx=config.pad_index)
    self.dropout = nn.Dropout(config.dropout)
    self.rnn = nn.LSTM(config.emb_size,
                       config.hidden_size,
                       config.num_layers,
                       batch_first=True, 
                       dropout=config.dropout)
    self.device = config.device
    self.output_dim = config.vocab_size
    self.output = nn.Linear(config.hidden_size, config.vocab_size)

  def forward(self, trg_input, hidden, cell):
    trg_input = trg_input.unsqueeze(1)
    text_embedded = self.dropout(self.embedding(trg_input))
    outputs, (hidden, cell) = self.rnn(text_embedded, (hidden, cell))
    outputs = outputs.squeeze(1)
    return self.output(outputs), hidden, cell

class Seq2Seq(nn.Module):

  def __init__(self, encoder, decoder, device):
    super().__init__()
    self.encoder = encoder
    self.decoder = decoder
    self.device = device

  def forward(self, 
              src_text, 
              src_text_lens,
              trg_text,
              trg_text_lens, 
              teacher_forcing_ratio=0.5):
    batch_size = src_text.size(0)
    max_trg_seq_len = trg_text.size(1)
    trg_vocab_size = self.decoder.output_dim

    outputs = torch.zeros(batch_size, max_trg_seq_len, trg_vocab_size, device=self.device)
    
    hidden, cell = self.encoder(src_text, src_text_lens)
    trg_input = trg_text[:, 0]

    for t in range(1, max_trg_seq_len):
      
      dec_output, hidden, cell = self.decoder(trg_input, hidden, cell)
      outputs[:, t, :] = dec_output

      teacher_force = random.random() < teacher_forcing_ratio

      top1 = dec_output.argmax(dim=1)
      trg_input = trg_text[:, t] if teacher_force else top1
      
    return outputs

encoder_config = Config(vocab_size=len(SRC.vocab),
                        emb_size=512,
                        hidden_size=256,
                        num_layers=2,
                        dropout=0.3,
                        pad_index=SRC.vocab.stoi[SRC.pad_token],
                        device=device)

decoder_config = Config(vocab_size=len(TRG.vocab),
                        emb_size=512,
                        hidden_size=256,
                        num_layers=2,
                        dropout=0.3,
                        pad_index=TRG.vocab.stoi[TRG.pad_token],
                        device=device)

encoder = Encoder(encoder_config)
decoder = Decoder(decoder_config)
seq2seq = Seq2Seq(encoder, decoder, device).to(device)

criterion = nn.CrossEntropyLoss(ignore_index=decoder_config.pad_index).to(device)
optimizer = torch.optim.Adam(seq2seq.parameters())

def train_epoch(model, iterator, optimizer, criterion):
  model.train()

  error = 0.
  for batch in iterator:
    optimizer.zero_grad()
    
    src_text, src_text_lens = batch.de
    src_text_lens = src_text_lens.cpu()
    trg_text, trg_text_lens = batch.en
    trg_text_lens = trg_text_lens.cpu()

    outputs = seq2seq(src_text, src_text_lens, trg_text, trg_text_lens)

    outputs = outputs[:, 1:, :]
    trg_text = trg_text[:, 1:]

    # outputs = outputs.contiguous()
    # trg_text = trg_text.contiguous()

    batch_size, seq_len, output_dim = outputs.shape

    outputs = outputs.reshape(batch_size * seq_len, output_dim)
    trg_text = trg_text.reshape(-1)

    loss = criterion(outputs, trg_text)
    loss.backward()
    optimizer.step()

    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

    error += loss.detach().cpu().numpy()

  return error / len(iterator)


def valid_epoch(model, iterator, criterion):
  model.eval()

  error = 0.
  with torch.no_grad():
    for batch in iterator:
      
      src_text, src_text_lens = batch.de
      src_text_lens = src_text_lens.cpu()
      trg_text, trg_text_lens = batch.en
      trg_text_lens = trg_text_lens.cpu()

      outputs = seq2seq(src_text, src_text_lens, trg_text, trg_text_lens)

      outputs = outputs[:, 1:, :]
      trg_text = trg_text[:, 1:]

      # outputs = outputs.contiguous()
      # trg_text = trg_text.contiguous()

      batch_size, seq_len, output_dim = outputs.shape

      outputs = outputs.reshape(batch_size * seq_len, output_dim)
      trg_text = trg_text.reshape(-1)

      loss = criterion(outputs, trg_text)

      error += loss.detach().cpu().numpy()

  return error / len(iterator)

for i in range(15):
  train_error = train_epoch(seq2seq, train_iterator, optimizer, criterion)
  valid_error = valid_epoch(seq2seq, valid_iterator, criterion)
  print(f'Epoch: {i + 1}, Train Error: {train_error}, Valid Error {valid_error}')

def translate_sentence(sentence, model, SRC, TRG, tokenizer, device, max_len=50):
  tokens = tokenizer(sentence)
  tokens = [SRC.init_token] + tokens + [TRG.eos_token]
  sentence_len = torch.LongTensor([len(tokens)])
  tokens_ids = [SRC.vocab.stoi[token] for token in tokens]
  token_tensor = torch.LongTensor([tokens_ids]).to(device)
  decoder_input = torch.LongTensor([TRG.vocab.stoi[TRG.init_token]]).to(device)

  pred_tokens = [TRG.vocab.stoi[TRG.init_token]]

  encoder = model.encoder
  decoder = model.decoder

  with torch.no_grad():
    hidden, cell = encoder(token_tensor, sentence_len)

  curr_len = 0
  while pred_tokens[-1] != TRG.vocab.stoi[TRG.eos_token] and pred_tokens[-1] != TRG.vocab.stoi['.'] and curr_len < max_len:
    with torch.no_grad():
      prediction, hidden, cell = decoder(decoder_input, hidden, cell)
    prediction = prediction.detach().cpu().numpy()
    pred_token_idx = np.argmax(prediction[0])
    pred_tokens.append(pred_token_idx)
    curr_len += 1
  result = ' '.join([TRG.vocab.itos[token] for token in pred_tokens[1:]])
  return result

print(translate_sentence('ein schwarzer hund und ein gefleckter hund kÃ¤mpfen .', seq2seq, SRC, TRG, tokenize_de, device))

